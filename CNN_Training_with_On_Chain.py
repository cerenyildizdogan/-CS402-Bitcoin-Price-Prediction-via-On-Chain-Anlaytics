# -*- coding: utf-8 -*-
"""CS402 CNN YiÄŸit

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nZebkMH1ZSAMfkrEVxlJWJm8matXUmW5

**Importing Libraries**
"""

import numpy as np
import tensorflow as tf
import pandas as pd
import math
import seaborn as sns
import matplotlib.pyplot as plt
import statsmodels.api as sm
from keras.models import Sequential, load_model, Model
from keras.layers import Dense, LSTM, GRU, Dropout, Input, Layer, Conv1D, Flatten, MaxPooling1D
from keras.optimizers import Adam
from keras.callbacks import EarlyStopping
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from datetime import date, timedelta, datetime 
from sklearn.feature_selection import RFE
from sklearn.linear_model import LinearRegression
from pandas.plotting import register_matplotlib_converters 
import matplotlib.dates as mdates 
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from keras import Model
import keras.backend as K
from google.colab import auth
auth.authenticate_user()
import gspread
from google.auth import default
sns.set_style('white', { 'axes.spines.right': False, 'axes.spines.top': False})
import math
import numpy as np
import pandas as pd
from sklearn.preprocessing import MinMaxScaler

"""**Uploading the Data**"""

"""
creds, _ = default()
gc = gspread.authorize(creds)
worksheet = gc.open('Selected OnChain Data (16/2/18 - 31/10/20)').sheet1
rows = worksheet.get_all_values()
df = pd.DataFrame.from_records(rows)
df.columns = df.iloc[0]
df = df.iloc[1:]
df.set_index("Date", inplace=True)
for col in df.columns:
    df[col] = df[col].astype('float32')
df
"""

import os
os.getcwd()
df = pd.read_excel("BTC Traditional Price Data (16_2_18 - 31_10_20).xlsx")
df = pd.DataFrame.from_records(df)
df.set_index('Date', inplace=True)
for col in df.columns:
    df[col] = df[col].astype('float32')
df

"""
train_data_unscaled = np.array(df)
nrows = train_data_unscaled.shape[0]

# Scaling
scaler = MinMaxScaler()
train_data_scaled = scaler.fit_transform(train_data_unscaled[:, :-1])  # Exclude the last column

# Scaling the 'BTC Price' separately
scaler_pred = MinMaxScaler()
np_Close_scaled = scaler_pred.fit_transform(train_data_unscaled[:, -1].reshape(-1, 1))  # Reshape to (nrows, 1)

# Partitioning
sequence_length = 30
index_Close = train_data_unscaled.shape[1] - 2 

train_data_len = math.ceil(train_data_scaled.shape[0] * 0.85)
train_data = train_data_scaled[:train_data_len, :]
test_data = train_data_scaled[train_data_len - sequence_length:, :]

# Partition the dataset efficiently
def partition_dataset(sequence_length, data):
    data_len = data.shape[0]
    indices = np.arange(sequence_length) + np.arange(data_len - sequence_length)[:, np.newaxis]
    x = data[indices]
    y = data[sequence_length:, index_Close]
    return x, y


x_train, y_train = partition_dataset(sequence_length, train_data)
x_test, y_test = partition_dataset(sequence_length, test_data)

# Reshape the input shape for the model
input_shape = (x_train.shape[1], x_train.shape[2])

print("X_train and y_train shapes: ", x_train.shape, y_train.shape)
print("x_test and y_test shapes: ", x_test.shape, y_test.shape)
"""

train_df = pd.DataFrame(df.copy())
train_df_ext = train_df.copy()
train_df_ext['Prediction'] = train_df_ext['BTC Price']
nrows = train_df.shape[0]
train_data_unscaled = np.array(train_df)
np_data = np.reshape(train_data_unscaled, (nrows, -1))
print("Train Data Set Shape: ", np_data.shape)

# Scaling
scaler = MinMaxScaler()
train_data_scaled = scaler.fit_transform(train_data_unscaled)
scaler_pred = MinMaxScaler()
df_Close = pd.DataFrame(train_df_ext['BTC Price'])
np_Close_scaled = scaler_pred.fit_transform(df_Close)

# Partitioning
sequence_length = 30
index_Close = train_df_ext.columns.get_loc("BTC Price")
train_data_len = math.ceil(train_data_scaled.shape[0] * 0.85)
train_data = train_data_scaled[0:train_data_len, :]
test_data = train_data_scaled[train_data_len - sequence_length:, :]

def partition_dataset(sequence_length, data):
    x, y = [], []
    data_len = data.shape[0]
    for i in range(sequence_length, data_len):
        x.append(data[i - sequence_length:i, :]) 
        y.append(data[i, index_Close])
    x = np.array(x)
    y = np.array(y)
    return x, y

x_train, y_train = partition_dataset(sequence_length, train_data)
x_test, y_test = partition_dataset(sequence_length, test_data)

print("X_train and y_train shapes: ", x_train.shape, y_train.shape)
print("x_test and y_test shapes: ", x_test.shape, y_test.shape)

# Define the input shape for the model
input_shape = (x_train.shape[1], x_train.shape[2])  # Shape without additional dimension

import time
model = Sequential()
model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=input_shape))
model.add(MaxPooling1D(pool_size=2))
model.add(Flatten())
model.add(Dense(64, activation='relu'))
model.add(Dense(1))  # Assuming you want to predict a single value

# Compile the model
start_time = time.time()
model.compile(optimizer='adam', loss='mse')


"""
from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization
from keras import regularizers
from keras.callbacks import EarlyStopping, ReduceLROnPlateau
from sklearn.model_selection import train_test_split


x_train, x_val, y_train, y_val = train_test_split(x_train_augmented, y_train_augmented, test_size=0.2, random_state=42)


model = Sequential()
model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=input_shape))
model.add(BatchNormalization())
model.add(MaxPooling1D(pool_size=2))
model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))
model.add(BatchNormalization())
model.add(MaxPooling1D(pool_size=2))
model.add(Conv1D(filters=256, kernel_size=3, activation='relu'))
model.add(BatchNormalization())
model.add(MaxPooling1D(pool_size=2))
model.add(Flatten())
model.add(Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.1)))
model.add(Dropout(0.5))
model.add(Dense(1))

# Compile the model
model.compile(optimizer='adam', loss='mse')
"""

"""
from keras.layers import Conv2D, GlobalMaxPooling2D
from keras.preprocessing.image import ImageDataGenerator
from keras import optimizers
import keras
import numpy as np

x_train = np.reshape(x_train, (-1, *input_shape, 1))
x_test = np.reshape(x_test, (-1, *input_shape, 1))

datagen = ImageDataGenerator(rotation_range=20, width_shift_range=0.1, height_shift_range=0.1, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)

# Generate augmented samples from the original data
augmented_generator = datagen.flow(x_train, y_train, batch_size=64)

# Define the model architecture
model = Sequential()
model.add(Conv2D(filters=64, kernel_size=3, activation='relu', input_shape=input_shape + (1,)))
model.add(GlobalMaxPooling2D())
model.add(Dense(50, activation='relu'))
model.add(Dense(1, activation='sigmoid'))

# Compile the model
learning_rate = 0.05  # Specify your desired learning rate
optimizer = keras.optimizers.Adam(learning_rate=learning_rate)
model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])

# Train the model using augmented data
history = model.fit(augmented_generator,
                    epochs=100,
                    validation_data=(x_test, y_test),
                    verbose=1)

"""

# Train the model
# Train the model using augmented data
"""
from keras.preprocessing.image import ImageDataGenerator
from keras.preprocessing.sequence import TimeseriesGenerator

# Define the data augmentation parameters
early_stopping = EarlyStopping(patience=20, restore_best_weights=True)
reduce_lr = ReduceLROnPlateau(factor=0.5, patience=5)


# Create an empty array to store augmented data
augmented_data = []

# Apply data augmentation techniques to each sample in the training data
for sample in x_train:
    # Randomly shift the sample in the time dimension
    shifted_sample = np.roll(sample, np.random.randint(0, len(sample)))
    
    # Randomly scale the sample
    scaled_sample = shifted_sample * np.random.uniform(0.9, 1.1)
    
    # Add random noise to the sample
    noisy_sample = scaled_sample + np.random.normal(0, 0.01, size=len(sample))
    
    augmented_data.append(noisy_sample)

# Convert the augmented data to a numpy array
augmented_data = np.array(augmented_data)

# Concatenate the augmented data with the original training data
x_train_augmented = np.concatenate((x_train, augmented_data))
y_train_augmented = np.concatenate((y_train, y_train))

# Train your model using the augmented data
history = model.fit(
    x_train_augmented,
    y_train_augmented,
    batch_size=64,
    epochs=200,
    validation_data=(x_val, y_val),
    callbacks=[early_stopping, reduce_lr]
)
"""
history = model.fit(x_train, y_train, batch_size=64, epochs=200, validation_data=(x_test, y_test), verbose=1)

# Evaluate the model
y_train_pred = model.predict(x_train)
y_test_pred = model.predict(x_test)
print("--- %s seconds ---" % (time.time() - start_time))

# Inverse scaling on the predictions
y_train_pred = scaler_pred.inverse_transform(y_train_pred)
y_test_pred = scaler_pred.inverse_transform(y_test_pred)
y_train_unscaled = scaler_pred.inverse_transform(y_train.reshape(-1, 1))
y_test_unscaled = scaler_pred.inverse_transform(y_test.reshape(-1, 1))

# Calculate evaluation metrics
r2 = r2_score(y_test_unscaled, y_test_pred)
print(f'R2: {np.round(r2, 4)}')

MAE = mean_absolute_error(y_test_unscaled, y_test_pred)
print(f'Mean Absolute Error (MAE): {np.round(MAE, 2)}')

MAPE = np.mean((np.abs(np.subtract(y_test_unscaled, y_test_pred) / y_test_unscaled))) * 100
print(f'Mean Absolute Percentage Error (MAPE): {np.round(MAPE, 2)}')

# The date from which on the date is displayed
display_start_date = "2018-01-01"

# Add the difference between the valid and predicted prices
train = pd.DataFrame(train_df_ext['BTC Price'][:train_data_len + 1]).rename(columns={'BTC Price': 'y_train'})
valid = pd.DataFrame(train_df_ext['BTC Price'][train_data_len:]).rename(columns={'BTC Price': 'y_test'})

# Get the predicted values from the model
y_pred = model.predict(x_test)
predictions_unscaled = scaler_pred.inverse_transform(y_pred.reshape(-1, 1))
valid.insert(1, "y_pred", predictions_unscaled, True)

valid.insert(1, "residuals", valid["y_pred"] - valid["y_test"], True)
df_union = pd.concat([train, valid])

# Zoom in to a closer timeframe
df_union_zoom = df_union[df_union.index > display_start_date]

# Create the lineplot
fig, ax1 = plt.subplots(figsize=(16, 8))
plt.title("y_pred vs y_test")
plt.ylabel("BTC Price", fontsize=18)
sns.set_palette(["#090364", "#1960EF", "#EF5919"])
sns.lineplot(data=df_union_zoom[['y_pred', 'y_train', 'y_test']], linewidth=1.0, dashes=False, ax=ax1)

# Create the bar plot with the differences
df_sub = ["#2BC97A" if x > 0 else "#C92B2B" for x in df_union_zoom["residuals"].dropna()]
ax1.bar(height=df_union_zoom['residuals'].dropna(), x=df_union_zoom['residuals'].dropna().index, width=3, label='residuals', color=df_sub)

plt.legend()
plt.show()
